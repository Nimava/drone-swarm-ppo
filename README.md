PPO-Based Optimization of Drone Swarm Dynamics

Authors: Tzyy-Leng Horng, Nima Vaziri
Repository: https://github.com/Nimava/drone-swarm-ppo

ğŸŒŸ Overview

This repository contains the implementation and simulation code for the research work:

â€œPPO-Based Optimization of Swarm Dynamics for Leaderâ€“Follower Drone Navigation and Collision Avoidance.â€

The project integrates classical swarm dynamics (cohesionâ€“alignmentâ€“separation) with Proximal Policy Optimization (PPO) to automatically tune swarm behavior in:

nozzle-shaped tunnels

cluttered jungle environments

moving-obstacle fields

The goal is to improve:

collision avoidance

formation stability

leaderâ€“follower performance

without manually tuning swarm parameters.

ğŸ“ Repository Structure (drone-swarm-ppo)
.
â”œâ”€â”€ src/                          # All source code (simulation + PPO training)
â”‚   â”œâ”€â”€ drone_swarm_env.py        # Gym-like environment for PPO
â”‚   â”œâ”€â”€ train_ppo.py              # PPO training script
â”‚   â”œâ”€â”€ run_trained_policy.py     # Run the trained PPO policy
â”‚   â”œâ”€â”€ simulate_jungle_case.py   # Simulation script for the "jungle" scenario
â”‚   â”œâ”€â”€ case01_nozzle_simulation.py
â”‚   â”œâ”€â”€ case02_straight_path.py
â”‚   â”œâ”€â”€ case03_dynamic_obstacles.py
â”‚   â”œâ”€â”€ ... (other case files)
â”‚
â”œâ”€â”€ docs/                         # Figures, GIFs, documentation images
â”‚   â”œâ”€â”€ nozzle_2d.png
â”‚   â”œâ”€â”€ nozzle_3d.png
â”‚   â”œâ”€â”€ trajectories.gif
â”‚   â”œâ”€â”€ ...
â”‚
â”œâ”€â”€ config.json                   # Configuration file for simulations (user-editable)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     # Project documentation

ğŸ”§ Installation

Install dependencies:

pip install -r requirements.txt

ğŸš€ Running Simulations
1. Simulations
python src/2d_nozzle.py
python src/3d_nozzle.py
python src/jungle.py
python src/moving_obstacle.py

2. Train PPO
python src/train_ppo.py

3. Evaluate trained PPO policy
python src/run_trained_policy.py

4. Run the optimized jungle-case
python src/simulate_jungle_case.py

ğŸ§  Summary of PPO Approach

The PPO agent learns optimal swarm coefficients:

kc â€“ cohesion

ka â€“ alignment

ks â€“ separation / obstacle avoidance

Actions:

[kc, ka, ks]


State includes:

average inter-drone distance

velocity alignment score

leaderâ€“drone lag

Reward penalizes collisions and instability, and rewards cohesion, alignment, and leader-following.

ğŸ“Š Results

Example outputs (in docs/):

2D + 3D nozzle geometry

PPO vs baseline trajectories

GIF animations of swarm movement

These demonstrate smoother trajectories and fewer collisions with PPO.

ğŸ“œ License

This repository is released under the MIT License.

ğŸ“„ CITATION

Because the paper is not yet published, do not include a CITATION.cff.
We will add it after acceptance.
